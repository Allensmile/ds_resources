# Deep Learning

## Terms

### Batch Normalization (BN)
- speeds up training by by preventing significant shifts in the distribution of inputs to each layer
- BN also allows us to significantly increase the learning rate
- BN acts like a **regularizer** and removes the need for **dropout** and **L2 regularization**
- batch regularization largely removes the need for for photometric distortions, and we can expose the network to more "real" images during the training process

### FC = fully connected

### Learning Rate Adaptation

#### AdaGrad


## References

- book: [Fundamentals of Deep Learning](http://shop.oreilly.com/product/0636920039709.do)
