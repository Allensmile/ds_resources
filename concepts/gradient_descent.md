# Gradient Descent

- learning rate is also known as step size

## Challenges in Finding Minimum(a)
- If a function has multiple minima, gradient descent may not find the global minimum
- If a function has no minimum, gradient descent may go on forever.

## BATCH Gradient Descent
- gradient is computed on the whole data set before taking a step.

## STOCHASTIC Gradient Descent
- uses one example (row of data) in each iteration

## MINI-BATCH Gradient Descent
- uses a small sample of the dataset to compute the gradient of the cost function




