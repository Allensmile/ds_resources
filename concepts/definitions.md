# Definitions 

### AUC
AUC of a random binary classifier is:  0.50

### ROC


### [Cross Entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)
Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.

### Entropy


### Ensemble Methods

### Bagging
**Bagging** means randomly extracting smaller datasets (we refer to them as bags of data) from the main training dataset and constructing a separate decision tree for each bag.

### Bootstrapping
- basic idea of bootstrapping is that you use your sample as a population. And from it you sample repeatedly, with replacement, to build other samples of the same size as your original sample.

### Key Performance Indicator (KPI)

### Pareto Principle | 80/20 rule
Pareto Principle, also known as the 80/20 rules, (the law of the vital few, and the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes.

### LOESS  (locally weighted scatterplot smoothing)
- LOWESS (Locally Weighted Scatterplot Smoothing), sometimes called LOESS (locally weighted smoothing), is a popular tool used in regression analysis that creates a smooth line through a timeplot or scatter plot to help you to see relationship between variables and foresee trends.

### SSD (Single Shot Multibox Detector)
